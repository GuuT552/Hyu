{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtLJlW4v5VF"
      },
      "source": [
        "## Домашнее задание №8\n",
        "\n",
        "В данном задании вам предстоит детально рассмотреть механизм Attention (и реализовать несколько его вариантов), а также вернуться к задаче классификации текстов из задания №6 и решить ее с использованием BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjC9pIjtpRKN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcmR3x9CpRKO"
      },
      "source": [
        "### Шаг №1. Реализация Attention\n",
        "\n",
        "В данной задаче вам предстоит реализовать механизм Attention, в частности несколько способов подсчета attention scores. Конечно, в популярных фреймворках данный механизм уже реализован, но для лучшего понимания вам предстаит реализовать его с помощью `numpy`.\n",
        "\n",
        "Ваше задание в данной задаче: реализовать `additive` (аддитивный) и `multiplicative` (мультипликативный) варианты Attention. Для вашего удобства (и для примера) `dot product` attention (основанный на скалярном произведении) уже реализован.\n",
        "\n",
        "Детальное описание данных типов Attention доступно в лекционных слайдах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxmpLaCzpRKO"
      },
      "outputs": [],
      "source": [
        "decoder_hidden_state = np.array([7, 11, 4]).astype(float)[:, None]\n",
        "\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.pcolormesh(decoder_hidden_state)\n",
        "plt.colorbar()\n",
        "plt.title('Decoder state')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbY_bYdupRKP"
      },
      "source": [
        "#### Dot product attention (пример реализации)\n",
        "Рассмотрим единственное состояние энкодера – вектор с размерностью `(n_hidden, 1)`, где `n_hidden = 3`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBkIDbabpRKP"
      },
      "outputs": [],
      "source": [
        "single_encoder_hidden_state = np.array([1, 5, 11]).astype(float)[:, None]\n",
        "\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.pcolormesh(single_encoder_hidden_state)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtDXXwV0pRKP"
      },
      "source": [
        "Attention score между данными состояниями энкодера и декодера вычисляются просто как скалярное произведение:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEZm6d_fpRKP"
      },
      "outputs": [],
      "source": [
        "np.dot(decoder_hidden_state.T, single_encoder_hidden_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hECv2EC9pRKQ"
      },
      "source": [
        "В общем случае состояний энкодера, конечно, несколько. Attention scores вычисляются с каждым из состояний энкодера:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGTBLEkWpRKV"
      },
      "outputs": [],
      "source": [
        "encoder_hidden_states = np.array([\n",
        "    [1, 5, 11],\n",
        "    [7, 4, 1],\n",
        "    [8, 12, 2],\n",
        "    [-9, 0, 1]\n",
        "\n",
        "]).astype(float).T\n",
        "\n",
        "encoder_hidden_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdUDmGrlpRKV"
      },
      "source": [
        "Тогда для подсчета скалярных произведений между единственным состоянием декодера и всеми состояниями энкодера можно воспользоваться следующей функцией (которая по факту представляет собой просто матричное умножение и приведение типов):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F72SP1iCpRKV"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention_score(decoder_hidden_state, encoder_hidden_states):\n",
        "    '''\n",
        "    decoder_hidden_state: np.array of shape (n_features, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features, n_states)\n",
        "\n",
        "    return: np.array of shape (1, n_states)\n",
        "        Array with dot product attention scores\n",
        "    '''\n",
        "    attention_scores = np.dot(decoder_hidden_state.T, encoder_hidden_states)\n",
        "    return attention_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLCbDkI7pRKV"
      },
      "outputs": [],
      "source": [
        "dot_product_attention_score(decoder_hidden_state, encoder_hidden_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57HntTOapRKW"
      },
      "source": [
        "Для подсчета \"весов\" нам необходим Softmax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTRgVaOMpRKW"
      },
      "outputs": [],
      "source": [
        "def softmax(vector):\n",
        "    '''\n",
        "    vector: np.array of shape (n, m)\n",
        "\n",
        "    return: np.array of shape (n, m)\n",
        "        Matrix where softmax is computed for every row independently\n",
        "    '''\n",
        "    nice_vector = vector - vector.max()\n",
        "    exp_vector = np.exp(nice_vector)\n",
        "    exp_denominator = np.sum(exp_vector, axis=1)[:, np.newaxis]\n",
        "    softmax_ = exp_vector / exp_denominator\n",
        "    return softmax_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IL_ceN7pRKW"
      },
      "outputs": [],
      "source": [
        "weights_vector = softmax(dot_product_attention_score(decoder_hidden_state, encoder_hidden_states))\n",
        "\n",
        "weights_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH_jQ5uppRKW"
      },
      "source": [
        "Наконец, воспользуемся данными весами и вычислим итоговый вектор, как и описано для dot product attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwwCpz4UpRKW"
      },
      "outputs": [],
      "source": [
        "attention_vector = weights_vector.dot(encoder_hidden_states.T).T\n",
        "print(attention_vector)\n",
        "\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.pcolormesh(attention_vector, cmap='spring')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wohEGJA6pRKW"
      },
      "source": [
        "Данный вектор аккумулирует в себе информацию из всех состояний энкодера, взвешенную на основе близости к заданному состоянию декодера.\n",
        "\n",
        "Реализуем все вышеописанные преобразования в единой функции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPNl-szepRKW"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(decoder_hidden_state, encoder_hidden_states):\n",
        "    '''\n",
        "    decoder_hidden_state: np.array of shape (n_features, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features, n_states)\n",
        "\n",
        "    return: np.array of shape (n_features, 1)\n",
        "        Final attention vector\n",
        "    '''\n",
        "    softmax_vector = softmax(dot_product_attention_score(decoder_hidden_state, encoder_hidden_states))\n",
        "    attention_vector = softmax_vector.dot(encoder_hidden_states.T).T\n",
        "    return attention_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktZ1HJgJpRKW"
      },
      "outputs": [],
      "source": [
        "assert (attention_vector == dot_product_attention(decoder_hidden_state, encoder_hidden_states)).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IPXJP9vpRKW"
      },
      "source": [
        "#### Multiplicative attention\n",
        "Ваша текущая задача: реализовать multiplicative attention.\n",
        "$$ e_i = \\mathbf{s}^TW_{mult}\\mathbf{h}_i $$\n",
        "\n",
        "Матрица весов `W_mult` задана ниже.\n",
        "Стоит заметить, что multiplicative attention позволяет работать с состояниями энкодера и декодера различных размерностей, поэтому состояния энкодера будут обновлены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hav5rr25pRKW"
      },
      "outputs": [],
      "source": [
        "encoder_hidden_states_complex = np.array([\n",
        "    [1, 5, 11, 4, -4],\n",
        "    [7, 4, 1, 2, 2],\n",
        "    [8, 12, 2, 11, 5],\n",
        "    [-9, 0, 1, 8, 12]\n",
        "\n",
        "]).astype(float).T\n",
        "\n",
        "W_mult = np.array([\n",
        "    [-0.78, -0.97, -1.09, -1.79,  0.24],\n",
        "    [ 0.04, -0.27, -0.98, -0.49,  0.52],\n",
        "    [ 1.08,  0.91, -0.99,  2.04, -0.15]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yInIvWA5pRKW"
      },
      "outputs": [],
      "source": [
        "# Implementation of multiplicative attention steps:\n",
        "# Variables expected to be in scope:\n",
        "# decoder_hidden_state: np.array of shape (n_features_dec, 1) -> (3,1)\n",
        "# encoder_hidden_states_complex: np.array of shape (n_features_enc, n_states) -> (5,4)\n",
        "# W_mult: np.array of shape (n_features_dec, n_features_enc) -> (3,5)\n",
        "# softmax: function defined previously\n",
        "# np: numpy library\n",
        "\n",
        "# 1. Calculate attention scores: e_i = s^T W_mult h_i\n",
        "# s.T @ W_mult is (1, n_features_dec) @ (n_features_dec, n_features_enc) = (1, n_features_enc)\n",
        "s_T_W = np.dot(decoder_hidden_state.T, W_mult)\n",
        "\n",
        "# (s.T @ W_mult) @ H_enc where H_enc is encoder_hidden_states_complex\n",
        "# attention_scores will be (1, n_states)\n",
        "attention_scores_step = np.dot(s_T_W, encoder_hidden_states_complex)\n",
        "\n",
        "# 2. Apply softmax to get weights\n",
        "# weights_vector_step will be (1, n_states)\n",
        "weights_vector_step = softmax(attention_scores_step)\n",
        "\n",
        "# 3. Compute the final attention vector: sum(weights_i * h_i)\n",
        "# This is encoder_hidden_states_complex @ weights_vector_step.T\n",
        "# (n_features_enc, n_states) @ (n_states, 1) = (n_features_enc, 1)\n",
        "attention_vector_step_result = np.dot(encoder_hidden_states_complex, weights_vector_step.T)\n",
        "\n",
        "print(\"--- Multiplicative Attention Steps Calculation (in cell yInIvWA5pRKW) ---\")\n",
        "print(\"decoder_hidden_state shape:\", decoder_hidden_state.shape)\n",
        "print(\"encoder_hidden_states_complex shape:\", encoder_hidden_states_complex.shape)\n",
        "print(\"W_mult shape:\", W_mult.shape)\n",
        "print(\"s_T_W (decoder_hidden_state.T @ W_mult) shape:\", s_T_W.shape)\n",
        "print(\"attention_scores_step shape:\", attention_scores_step.shape)\n",
        "print(\"weights_vector_step shape:\", weights_vector_step.shape)\n",
        "print(\"attention_vector_step_result shape:\", attention_vector_step_result.shape)\n",
        "print(\"\\nResulting attention_vector_step_result:\\n\", attention_vector_step_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IfWyrQPpRKW"
      },
      "source": [
        "Реализуйте подсчет attention согласно формулам и реализуйте итоговую функцию `multiplicative_attention`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBuGAOfupRKW"
      },
      "outputs": [],
      "source": [
        "def multiplicative_attention(decoder_hidden_state, encoder_hidden_states, W_mult):\n",
        "    '''\n",
        "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
        "    W_mult: np.array of shape (n_features_dec, n_features_enc)\n",
        "\n",
        "    return: np.array of shape (n_features_enc, 1)\n",
        "        Final attention vector\n",
        "    '''\n",
    # 1. Calculate part1 = W_add_enc @ encoder_hidden_states
    # W_add_enc: (n_features_int, n_features_enc)
    # encoder_hidden_states: (n_features_enc, n_states)
    # part1: (n_features_int, n_states)
    part1 = np.dot(W_add_enc, encoder_hidden_states)

    # 2. Calculate part2 = W_add_dec @ decoder_hidden_state
    # W_add_dec: (n_features_int, n_features_dec)
    # decoder_hidden_state: (n_features_dec, 1)
    # part2: (n_features_int, 1)
    part2 = np.dot(W_add_dec, decoder_hidden_state)

    # 3. Combine part1 and part2 using broadcasting
    # combined: (n_features_int, n_states)
    combined = part1 + part2

    # 4. Apply np.tanh
    # activated: (n_features_int, n_states)
    activated = np.tanh(combined)

    # 5. Calculate attention scores: v_add.T @ activated
    # v_add.T: (1, n_features_int)
    # attention_scores: (1, n_states)
    attention_scores = np.dot(v_add.T, activated)

    # 6. Apply softmax to get weights_vector
    # weights_vector: (1, n_states)
    weights_vector = softmax(attention_scores) # Assuming softmax is defined and imported

    # 7. Compute the final attention_vector
    # encoder_hidden_states: (n_features_enc, n_states)
    # weights_vector.T: (n_states, 1)
    # attention_vector_result: (n_features_enc, 1)
    attention_vector_result = np.dot(encoder_hidden_states, weights_vector.T)
    
    return attention_vector_result
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi4TdrxYpRKW"
      },
      "source": [
        "#### Additive attention\n",
        "Теперь вам предстоит реализовать additive attention.\n",
        "\n",
        "$$ e_i = \\mathbf{v}^T \\text{tanh} (W_{add-enc} \\mathbf{h}_i + W_{add-dec} \\mathbf{s}) $$\n",
        "\n",
        "Матрицы весов `W_add_enc` и `W_add_dec` доступны ниже, как и вектор весов `v_add`. Для вычисления активации можно воспользоваться `np.tanh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD0CqZPepRKX"
      },
      "outputs": [],
      "source": [
        "v_add = np.array([[-0.35, -0.58,  0.07,  1.39, -0.79, -1.78, -0.35]]).T\n",
        "\n",
        "W_add_enc = np.array([\n",
        "    [-1.34, -0.1 , -0.38,  0.12, -0.34],\n",
        "    [-1.  ,  1.28,  0.49, -0.41, -0.32],\n",
        "    [-0.39, -1.38,  1.26,  1.21,  0.15],\n",
        "    [-0.18,  0.04,  1.36, -1.18, -0.53],\n",
        "    [-0.23,  0.96,  1.02,  0.39, -1.26],\n",
        "    [-1.27,  0.89, -0.85, -0.01, -1.19],\n",
        "    [ 0.46, -0.12, -0.86, -0.93, -0.4 ]\n",
        "])\n",
        "\n",
        "W_add_dec = np.array([\n",
        "    [-1.62, -0.02, -0.39],\n",
        "    [ 0.43,  0.61, -0.23],\n",
        "    [-1.5 , -0.43, -0.91],\n",
        "    [-0.14,  0.03,  0.05],\n",
        "    [ 0.85,  0.51,  0.63],\n",
        "    [ 0.39, -0.42,  1.34],\n",
        "    [-0.47, -0.31, -1.34]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-D78r9WpRKX"
      },
      "outputs": [],
      "source": [
        "# Implementation of additive attention steps:\n",
        "# Variables expected to be in scope:\n",
        "# decoder_hidden_state: (3,1)\n",
        "# encoder_hidden_states_complex: (5,4) - used as encoder_hidden_states for this calculation\n",
        "# v_add: (7,1)\n",
        "# W_add_enc: (7,5)\n",
        "# W_add_dec: (7,3)\n",
        "# softmax: function defined previously\n",
        "# np: numpy library\n",
        "\n",
        "print(\"--- Additive Attention Steps Calculation (in cell X-D78r9WpRKX) ---\")\n",
        "print(\"Using encoder_hidden_states_complex for encoder_hidden_states.\")\n",
        "print(\"decoder_hidden_state shape:\", decoder_hidden_state.shape)\n",
        "print(\"encoder_hidden_states_complex shape:\", encoder_hidden_states_complex.shape)\n",
        "print(\"v_add shape:\", v_add.shape)\n",
        "print(\"W_add_enc shape:\", W_add_enc.shape)\n",
        "print(\"W_add_dec shape:\", W_add_dec.shape)\n",
        "\n",
        "# 1. Calculate part1 = W_add_enc @ encoder_hidden_states_complex\n",
        "part1_step = np.dot(W_add_enc, encoder_hidden_states_complex) # (7,5) @ (5,4) -> (7,4)\n",
        "print(\"part1_step shape:\", part1_step.shape)\n",
        "\n",
        "# 2. Calculate part2 = W_add_dec @ decoder_hidden_state\n",
        "part2_step = np.dot(W_add_dec, decoder_hidden_state) # (7,3) @ (3,1) -> (7,1)\n",
        "print(\"part2_step shape:\", part2_step.shape)\n",
        "\n",
        "# 3. Combine part1 and part2 using broadcasting\n",
        "combined_step = part1_step + part2_step # (7,4) + (7,1) -> (7,4)\n",
        "print(\"combined_step shape:\", combined_step.shape)\n",
        "\n",
        "# 4. Apply np.tanh\n",
        "activated_step = np.tanh(combined_step) # (7,4)\n",
        "print(\"activated_step shape:\", activated_step.shape)\n",
        "\n",
        "# 5. Calculate attention scores: v_add.T @ activated\n",
        "attention_scores_add_step = np.dot(v_add.T, activated_step) # (1,7) @ (7,4) -> (1,4)\n",
        "print(\"attention_scores_add_step shape:\", attention_scores_add_step.shape)\n",
        "\n",
        "# 6. Apply softmax to get weights_vector\n",
        "weights_vector_add_step = softmax(attention_scores_add_step) # (1,4)\n",
        "print(\"weights_vector_add_step shape:\", weights_vector_add_step.shape)\n",
        "\n",
        "# 7. Compute the final attention_vector\n",
        "# Using encoder_hidden_states_complex as the source for weighted sum\n",
        "attention_vector_add_step_result = np.dot(encoder_hidden_states_complex, weights_vector_add_step.T) # (5,4) @ (4,1) -> (5,1)\n",
        "print(\"attention_vector_add_step_result shape:\", attention_vector_add_step_result.shape)\n",
        "print(\"\\nResulting attention_vector_add_step_result:\\n\", attention_vector_add_step_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrr_hibjpRKX"
      },
      "source": [
        "Реализуйте подсчет attention согласно формулам и реализуйте итоговую функцию `additive_attention`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqE6OsOKpRKX"
      },
      "outputs": [],
      "source": [
        "def additive_attention(decoder_hidden_state, encoder_hidden_states, v_add, W_add_enc, W_add_dec):\n",
        "    '''\n",
        "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
        "    v_add: np.array of shape (n_features_int, 1)\n",
        "    W_add_enc: np.array of shape (n_features_int, n_features_enc)\n",
        "    W_add_dec: np.array of shape (n_features_int, n_features_dec)\n",
        "\n",
        "    return: np.array of shape (n_features_enc, 1)\n",
        "        Final attention vector\n",
        "    '''\n",
    # Calculate attention scores
    # decoder_hidden_state.T: (1, n_features_dec)
    # W_mult: (n_features_dec, n_features_enc)
    # encoder_hidden_states: (n_features_enc, n_states)
    # attention_scores: (1, n_states)
    # Using np.dot for clarity and consistency with dot_product_attention_score example
    attention_scores = np.dot(decoder_hidden_state.T, W_mult) # Intermediate: (1, n_features_enc)
    attention_scores = np.dot(attention_scores, encoder_hidden_states) # Final: (1, n_states)

    # Apply softmax to get weights
    # weights_vector: (1, n_states)
    weights_vector = softmax(attention_scores) # Assuming softmax is defined and imported

    # Compute the final attention vector
    # encoder_hidden_states: (n_features_enc, n_states)
    # weights_vector.T: (n_states, 1)
    # attention_vector_result: (n_features_enc, 1)
    attention_vector_result = np.dot(encoder_hidden_states, weights_vector.T)
    
    return attention_vector_result
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSL0r2b8pRKX"
      },
      "source": [
        "Сдайте функции `multiplicative_attention` и `additive_attention` в контест.\n",
        "\n",
        "Не забудьте про импорт `numpy`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DaAq6cCpRKX"
      },
      "source": [
        "### Шаг №2 (опциональный). Классификация текстов с использованием предобученной языковой модели.\n",
        "\n",
        "Вновь вернемся к набору данных SST-2. Разобьем выборку на train и test аналогично заданию №6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4jcBRSFpRKX"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/msu_branch/homeworks/hw08_attention/holdout_texts08.npy\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF-Tj05kpRKX"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "df = pd.read_csv(\n",
        "    'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv',\n",
        "    delimiter='\\t',\n",
        "    header=None\n",
        ")\n",
        "texts_train = df[0].values[:5000]\n",
        "y_train = df[1].values[:5000]\n",
        "texts_test = df[0].values[5000:]\n",
        "y_test = df[1].values[5000:]\n",
        "texts_holdout = np.load('holdout_texts08.npy', allow_pickle=True)\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-58S6DJRpRKX"
      },
      "source": [
        "Весь остальной код предстоит написать вам.\n",
        "\n",
        "Для успешной сдачи на максимальный балл необходимо добиться хотя бы __84.5% accuracy на тестовой части выборки__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MrxrQm_pRKX"
      },
      "outputs": [],
      "source": [
        "# your beautiful experiments here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymNTw91upRKX"
      },
      "source": [
        "#### Сдача взадания в контест\n",
        "Сохраните в словарь `out_dict` вероятности принадлежности к нулевому и первому классу соответственно:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqDVO08UpRKY"
      },
      "outputs": [],
      "source": [
        "out_dict = {\n",
        "    'train': # np.array of size (5000, 2) with probas\n",
        "    'test': # np.array of size (1920, 2) with probas\n",
        "    'holdout': # np.array of size (500, 2) with probas\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Kz4ybfopRKa"
      },
      "source": [
        "Несколько `assert`'ов для проверки вашей посылки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4I5nN4wpRKa"
      },
      "outputs": [],
      "source": [
        "assert isinstance(out_dict['train'], np.ndarray), 'Dict values should be numpy arrays'\n",
        "assert out_dict['train'].shape == (5000, 2), 'The predicted probas shape does not match the train set size'\n",
        "assert np.allclose(out_dict['train'].sum(axis=1), 1.), 'Probas do not sum up to 1 for some of the objects'\n",
        "\n",
        "assert isinstance(out_dict['test'], np.ndarray), 'Dict values should be numpy arrays'\n",
        "assert out_dict['test'].shape == (1920, 2), 'The predicted probas shape does not match the test set size'\n",
        "assert np.allclose(out_dict['test'].sum(axis=1), 1.), 'Probas do not sum up to 1 for some of the object'\n",
        "\n",
        "assert isinstance(out_dict['holdout'], np.ndarray), 'Dict values should be numpy arrays'\n",
        "assert out_dict['holdout'].shape == (500, 2), 'The predicted probas shape does not match the holdout set size'\n",
        "assert np.allclose(out_dict['holdout'].sum(axis=1), 1.), 'Probas do not sum up to 1 for some of the object'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQRsk-8zpRKa"
      },
      "source": [
        "Запустите код ниже для генерации посылки и сдайте файл `submission_dict_hw08.npy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6N8xnRhpRKa"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "np.save('submission_dict_hw08.npy', out_dict, allow_pickle=True)\n",
        "print('File saved to `submission_dict_hw08.npy`')\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTKHfkYFpRKa"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP_hw01_texts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}